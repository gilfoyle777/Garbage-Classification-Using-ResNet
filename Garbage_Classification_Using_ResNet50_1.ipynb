{"cells":[{"metadata":{"_uuid":"325aaa22-dac7-43b9-ab14-adcc39723af3","_cell_guid":"cee1e73f-8077-41f3-aa43-131cc98cfbf0","trusted":true},"cell_type":"markdown","source":"> # Garbage Classification using PyTorch\n\nIn this kernel, we'll classify waste into various categories using Residual Networks in PyTorch. The dataset being used: [Garbage_data](https://www.kaggle.com/asdasdasasdas/garbage-classification)\n\n","execution_count":null},{"metadata":{"_uuid":"7e0e8141-0ba9-4f46-9941-90d552b7ac2c","_cell_guid":"39a9a5a7-7690-4994-ab16-fd8c89c4929b","trusted":true},"cell_type":"code","source":"import os\nimport torch\nimport torchvision\nfrom torchvision.datasets.utils import download_url\nfrom torch.utils.data import random_split\nimport torchvision.models as models\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport time\nimport copy\nimport glob\nimport torchvision.transforms as transforms\nimport torch.optim as optim","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7a891e9-13eb-4c6f-8285-80f3f1867611","_cell_guid":"1a95fd46-01a5-492c-aaaf-ec456b20c1cc","trusted":true},"cell_type":"code","source":"data_dir = '../input/garbage-classification/garbage classification/Garbage classification'\n\nprint(os.listdir(data_dir))\nclasses = os.listdir(data_dir)\nprint(\"\\nClasses:\", classes)\nprint(\"\\nNumber of Classes:\", len(classes))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"92816e19-4cb2-4b81-ad27-fc685ac29eb5","_cell_guid":"b304421c-6413-405f-a771-1c82d92da48f","trusted":true},"cell_type":"markdown","source":"## Image Augmentation:\n\nWe'll be augmenting images using `torchvision.transforms`, which helps in reducing the chances of overfitting or the inability to generalize on new data.\n\nThis kernel uses images of size `128x128` after augmentation due to GPU restrictions, but feel free to switch to a higher resolution if you have access to more resources!","execution_count":null},{"metadata":{"_uuid":"4fd65b37-434f-4e24-951d-817b2dd095e2","_cell_guid":"3e5efc5d-4766-47ed-ac10-f30c0b0aea7c","trusted":true},"cell_type":"code","source":"from torchvision.datasets import ImageFolder\nimport torchvision.transforms as transforms","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a032d66e-cdf4-444b-a9fb-ab786428889f","_cell_guid":"c785b851-bfdb-47eb-9813-4ab92f458d05","trusted":true},"cell_type":"code","source":"transformations = transforms.Compose([\n                        transforms.Resize((128, 128)),\n                        transforms.ToTensor(),\n                       ])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac5de45d-7c6e-4a96-a608-d80171a6b450","_cell_guid":"37cbd120-b7b4-4dfd-b109-38e320129c72","trusted":true},"cell_type":"code","source":"dataset = ImageFolder(data_dir, transform=transformations)\nprint(len(dataset))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d388574-e6f1-4cf3-90d3-10e68387d687","_cell_guid":"d48d06b7-ef3b-45f4-9598-2d00c94578a8","trusted":true},"cell_type":"code","source":"img, label = dataset[0]\nprint(img.shape, label)\nimg","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60db2fbc-6c23-4d7a-a6dc-e56cdbf224db","_cell_guid":"578fae06-579a-4740-8e5e-abc807bdd833","trusted":true},"cell_type":"markdown","source":"## Visualizing Samples:\n\nHere is a helper function for visualizing sample images:","execution_count":null},{"metadata":{"_uuid":"0535be41-93fe-4587-8d8d-939a1a088b87","_cell_guid":"89a77529-4f97-4e92-ada2-82330f58f56b","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\ndef show_example(img, label):\n    print('Label: ', dataset.classes[label], \"(\"+str(label)+\")\")\n    plt.imshow(img.permute(1, 2, 0))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de74ab52-343b-4540-b7c1-b4d85b65f6b9","_cell_guid":"2b5621df-11e4-42f3-a800-9e2a0a33fe25","trusted":true},"cell_type":"markdown","source":"Here is the first image of the dataset with its label:","execution_count":null},{"metadata":{"_uuid":"b504a8d1-fadd-4780-a41a-6d38c504fadc","_cell_guid":"6db5d452-c327-42a7-aeef-37f537974c86","trusted":true},"cell_type":"code","source":"img, label = dataset[0]\nshow_example(img, label)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"26813a6c-7ee5-4256-9352-4994c9fd7597","_cell_guid":"04472818-6954-4cf6-8b59-a5c052f62a52","trusted":true},"cell_type":"code","source":"show_example(*dataset[2000])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63099195-8e9e-453a-a270-e36502d5ad0f","_cell_guid":"bc69eb9d-a515-41ef-aba7-6e7d9635592d","trusted":true},"cell_type":"markdown","source":"## Loading Data:\n\nLet's start by creating training and validation sets. We'll be using 90% of the data for training and the rest for validation.","execution_count":null},{"metadata":{"_uuid":"19f682d1-c131-4163-b732-506962b92594","_cell_guid":"299c5160-faa7-43fb-aae5-5b92596bda98","trusted":true},"cell_type":"code","source":"random_seed = 42\ntorch.manual_seed(random_seed);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d9579f0-7846-46c7-a092-4bc94d87c3ed","_cell_guid":"6cf6c5dc-a057-4ee9-ae2a-b1c83632e129","trusted":true},"cell_type":"code","source":"len(dataset)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77361db9-d6ca-4edb-a376-f22a4f2c31c5","_cell_guid":"80b8e5fc-f6f3-4a36-8d81-6b8d4f79bd1a","trusted":true},"cell_type":"code","source":"val_per = 0.1\ntrain_size = len(dataset) - int(val_per*len(dataset))\nval_size = int(val_per*len(dataset))\n\n\ntrain_ds, val_ds = random_split(dataset, [train_size, val_size])\nlen(train_ds), len(val_ds)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f9da36d8-2c46-472b-985d-6f3abf575472","_cell_guid":"02f935a9-ea64-495f-8ca9-e3c40d53e0d5","trusted":true},"cell_type":"markdown","source":"We can now create data loaders for training and validation, to load the data in batches.\n\nThis kernel uses a batch size of 32; you can increase the batch size if you have VRAM <16GB.","execution_count":null},{"metadata":{"_uuid":"90bc2e57-c27d-4a1c-b8c5-0d03e102ab1f","_cell_guid":"404fe95b-8067-44cf-be13-7815dedd1d3d","trusted":true},"cell_type":"code","source":"from torch.utils.data.dataloader import DataLoader\n\nbatch_size= 32","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e9342e2-16b8-4655-9d9c-64c43f797ce0","_cell_guid":"02cee81c-d812-43f0-8c06-85d390481cec","trusted":true},"cell_type":"code","source":"train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\nval_dl = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9128e2e9-60bd-4e95-b9f9-aff16fa65a08","_cell_guid":"a549027b-d79c-4937-8a75-3d0e432ee307","trusted":true},"cell_type":"code","source":"from torchvision.utils import make_grid\n\ndef show_batch(dl):\n    for images, labels in dl:\n        fig, ax = plt.subplots(figsize=(12, 6))\n        ax.set_xticks([]); ax.set_yticks([])\n        ax.imshow(make_grid(images, nrow=16).permute(1, 2, 0))\n        break","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"09d05c58-a815-4690-819f-0ea09dba5bbb","_cell_guid":"c23aea43-131e-453c-85bb-2a64aa5a2286","trusted":true},"cell_type":"code","source":"show_batch(train_dl)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68c80b1a-e4cb-4a37-8184-f6044e128ce2","_cell_guid":"79975e7d-3473-4045-8769-610588fc8ab9","trusted":true},"cell_type":"markdown","source":"## Model:\n\nHere is the base of our classification model:","execution_count":null},{"metadata":{"_uuid":"af8405f5-c45b-49cc-9235-df59b7691bd1","_cell_guid":"5bc94159-d799-4ee2-b04e-6cffc9139964","trusted":true},"cell_type":"code","source":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n\nclass ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['train_loss'], result['val_loss'], result['val_acc']))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81a83aab-5126-4eff-b6bb-762ebbb0cabe","_cell_guid":"182e2e0f-18e9-472c-8265-691134010ed9","trusted":true},"cell_type":"code","source":"class ResNet(ImageClassificationBase):\n    def __init__(self):\n        super().__init__()\n        # Use a pretrained model\n        self.network = models.resnet50(pretrained=True)\n        # Replace last layer\n        num_ftrs = self.network.fc.in_features\n        self.network.fc = nn.Linear(num_ftrs, 87)\n    \n    def forward(self, xb):\n        return torch.sigmoid(self.network(xb))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b6221cc-db59-44ca-8808-1ea1861be5c0","_cell_guid":"25c984f3-b97a-4010-8295-3cef7c47918d","trusted":true},"cell_type":"code","source":"model = ResNet()\nmodel","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6eb9e55-8ca1-49cb-ad57-87763fd80dbf","_cell_guid":"9e5e870f-8124-4061-ae59-f997ae70fb41","trusted":true},"cell_type":"code","source":"for images, labels in train_dl:\n    print('images.shape:', images.shape)\n    out = model(images)\n    print('out.shape:', out.shape)\n    print('out[0]:', out[0])\n    break","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea02f994-9a5b-4444-86e7-3e46689d296a","_cell_guid":"35725967-36b9-4656-ba86-519ee9f456e4","trusted":true},"cell_type":"markdown","source":"These functions help in working with CUDA. I would personally recommend to train the model on GPUs due to their more processing power and faster training times.","execution_count":null},{"metadata":{"_uuid":"f452a33f-c05d-4434-a42c-5fba9913d1bb","_cell_guid":"5fbf45c2-0a40-42f3-a0d5-9903a68bcbf3","trusted":true},"cell_type":"code","source":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ab0ab00-b403-4b65-adef-600817272d25","_cell_guid":"62174a42-33ee-4747-a852-458fe9c7e45a","trusted":true},"cell_type":"code","source":"device = get_default_device()\ndevice","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17d34e2b-68aa-4939-bb39-4ac40676e5cc","_cell_guid":"3ab51d72-d70b-4765-9666-22786d632f67","trusted":true},"cell_type":"markdown","source":"We can now wrap our training and validation data loaders using `DeviceDataLoader` for automatically transferring batches of data to the GPU (if available), and use `to_device` to move our model to the GPU (if available).","execution_count":null},{"metadata":{"_uuid":"2a5edba9-94e9-4b25-a26d-b93a2290c09e","_cell_guid":"ae5cc347-e2c5-44c6-9971-6e0d7356e480","trusted":true},"cell_type":"code","source":"train_dl = DeviceDataLoader(train_dl, device)\nval_dl = DeviceDataLoader(val_dl, device)\nto_device(model, device);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"95e81f34-4bb3-4eaa-82cc-d47518ca6127","_cell_guid":"6eef5b1e-e665-42ec-ab81-6bad306c4602","trusted":true},"cell_type":"code","source":"@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        # Training Phase \n        model.train()\n        train_losses = []\n        for batch in train_loader:\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aaa5599e-3f65-478d-9312-7506b931d125","_cell_guid":"ba68ba7b-1745-446c-8a1b-5994b7727e1e","trusted":true},"cell_type":"markdown","source":"Before we begin training, let's instantiate the model once again and see how it performs on the validation set with the initial set of parameters.","execution_count":null},{"metadata":{"_uuid":"b1ff5f44-8f8c-469b-b1a8-028630e48654","_cell_guid":"e37d877b-383d-4d12-94af-4a3a560ea997","trusted":true},"cell_type":"code","source":"model = to_device(ResNet(), device)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3393c13a-65c1-4f91-b55f-9fe295c15986","_cell_guid":"b4f166f0-23c9-4008-a350-a79c9f29a59b","trusted":true},"cell_type":"code","source":"evaluate(model, val_dl)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5fe77f4-3f5d-46a3-ac2a-12a7d8dce51d","_cell_guid":"d975513f-9b42-4e2f-9025-69457be5959e","trusted":true},"cell_type":"markdown","source":"## Training the Model:","execution_count":null},{"metadata":{"_uuid":"fd647e1c-d9e2-4482-b69d-32b47b1c0134","_cell_guid":"8fc0fe21-c2ab-4cd8-a02e-06bfc980d06f","trusted":true},"cell_type":"code","source":"num_epochs = 3\nopt_func = torch.optim.Adam\nlr = 5.5e-5","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"925bb658-f28a-4cd5-ac53-66be32023b03","_cell_guid":"a3e0a57a-a881-4005-975d-e4f2b13391cd","trusted":true},"cell_type":"markdown","source":"Let's train the model now!","execution_count":null},{"metadata":{"_uuid":"5af256c7-d29a-4762-ba99-7ec73a1c102e","_cell_guid":"0ddcb0eb-34f8-4bf1-8435-e6b5dee95654","trusted":true},"cell_type":"code","source":"history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89d83d4e-823e-41e1-af28-ddbb8719336b","_cell_guid":"802876c3-6f4a-423a-bbfa-8224ab6bafaa","trusted":true},"cell_type":"markdown","source":"We can also plot the valdation set accuracies to study how the model improves over time.","execution_count":null},{"metadata":{"_uuid":"ebe82842-6c98-4dbc-9c5c-969c37b7c8f9","_cell_guid":"551ca639-e902-4c10-858e-41a47a5c7f52","trusted":true},"cell_type":"code","source":"num_epochs = 25\nopt_func = torch.optim.Adam\nlr = 6e-6","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5f372ac-9f8e-4ec1-aa6c-35b0bcc1479a","_cell_guid":"0dbfd4f5-bc51-4c81-b2e2-4bd7bab510b9","trusted":true},"cell_type":"code","source":"history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b60d677-5882-4d7d-b6bb-c90b532115a3","_cell_guid":"f79a0b2a-6c41-4c80-a533-d675ecc8e9cc","trusted":true},"cell_type":"markdown","source":"## Accuracy & Loss Plots:","execution_count":null},{"metadata":{"_uuid":"8d64a550-4997-486d-95f4-44a2447396b3","_cell_guid":"db039946-0699-40bd-ac6c-a4568c1842a8","trusted":true},"cell_type":"code","source":"def plot_accuracies(history):\n    accuracies = [x['val_acc'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4cadcc3-8df1-4e85-b40c-debfc283260f","_cell_guid":"e58cad8f-758c-45ad-a70a-9aaef2310435","trusted":true},"cell_type":"code","source":"plot_accuracies(history)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"362fe807-0b6b-4e7b-aae8-4874b3492ba9","_cell_guid":"62cf4062-59f6-4ff2-ae25-5f09e5bc15d7","trusted":true},"cell_type":"code","source":"def plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14700ae6-93d2-4a4c-9a12-790483213476","_cell_guid":"3b8cc01f-1d91-4b90-9795-dea4517ca9c0","trusted":true},"cell_type":"code","source":"plot_losses(history)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e80cd259-caf2-45f9-967c-976d9b6fa34e","_cell_guid":"f606ed80-3add-4f92-b43a-573cac8409db","trusted":true},"cell_type":"markdown","source":"## Making Predictions:","execution_count":null},{"metadata":{"_uuid":"9c139c61-4a49-4169-a439-df6e7d6facc8","_cell_guid":"07f19476-cc70-45a9-9730-20eec259b0a2","trusted":true},"cell_type":"code","source":"base_path='../input/data-test'\ntest_dataset = ImageFolder(base_path+'/TEST', transform=transformations)\ntest_dataset","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ee68ee0-c550-4813-91ad-0f4d0f0515e5","_cell_guid":"3aa25661-d32b-4780-9e0c-2a2458e5ffe9","trusted":true},"cell_type":"markdown","source":"Let's define a helper function `predict_image`, which returns the predicted label for a single image tensor.","execution_count":null},{"metadata":{"_uuid":"1cb43941-7ab0-4388-98d3-39ac0647b9fd","_cell_guid":"636e654b-383d-46f4-8c0c-a52e8f70f4e1","trusted":true},"cell_type":"code","source":"def predict_image(img, model):\n    # Convert to a batch of 1\n    xb = to_device(img.unsqueeze(0), device)\n    # Get predictions from model\n    yb = model(xb)\n    # Pick index with highest probability\n    _, preds  = torch.max(yb, dim=1)\n    # Retrieve the class label\n    return dataset.classes[preds[0].item()]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b2196dc9-410f-4633-90e1-1635bf8c4ba4","_cell_guid":"7dbe4e5d-ed41-49f3-8efe-0be65cc894f9","trusted":true},"cell_type":"code","source":"img, label = test_dataset[44]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ef87553-0aaa-4d2f-bc33-3d5389512694","_cell_guid":"690df8b2-3cce-46ed-b857-4ac0568ebdd6","trusted":true},"cell_type":"code","source":"img, label = test_dataset[14 2, 0))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df8b4a17-cd0a-4ba5-b787-120211d4a1d1","_cell_guid":"effa52ad-089f-478e-a2a1-9cca774fc899","trusted":true},"cell_type":"code","source":"img, label = test_dataset[560]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5acab65e-eccf-421c-a0b9-61603472b4d1","_cell_guid":"fc81e554-627d-4f58-bc1d-ca1058623672","trusted":true},"cell_type":"code","source":"test_loader = DeviceDataLoader(DataLoader(test_dataset, batch_size*2), device)\nresult = evaluate(model, test_loader)\nresult","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4fb422b2-c8a0-4e09-8251-0d604a3353b4","_cell_guid":"b39fae45-8bac-4c46-8f64-8eb1251b3b90","trusted":true},"cell_type":"code","source":"torch.save(model.state_dict(), 'waste-classification-data.pth')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c620ca2f-fd06-4204-ad1f-d0288c0b1492","_cell_guid":"413a2405-a470-4917-9035-226de4c7e899","trusted":true},"cell_type":"markdown","source":"## Testing on Downloaded Images:\n\nI'll be downloading files using `urllib` as for uploading files in Kaggle, you'll need to create a new dataset (which is not suitable for a few images).\n\nI'll be downloading an image of an plastic bottle:","execution_count":null},{"metadata":{"_uuid":"f350a9db-8720-49f3-90f1-6f285d8034cc","_cell_guid":"dd7fab99-a8a1-44fc-8331-0a56718c8873","trusted":true},"cell_type":"code","source":"import urllib.request\n# urllib.request.urlretrieve(\"data:image/jpeg;base64,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\", \"plastic.jpg\")\nurllib.request.urlretrieve(\"data:image/jpeg;base64,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\", \"plastic.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36a7fa5c-f277-4056-9e11-b4298732e42d","_cell_guid":"dd5d74fa-a1d8-44ca-b5cd-b862d3875de1","trusted":true},"cell_type":"markdown","source":"Let us load the pretrained model:","execution_count":null},{"metadata":{"_uuid":"1de2a02e-1c19-415b-a612-f6cfb8fa83e1","_cell_guid":"ad817c4e-0e7a-4800-890e-4b5b37d6744f","trusted":true},"cell_type":"code","source":"loaded_model = model\nloaded_model.load_state_dict(torch.load('./waste-classification-data.pth'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cda7d764-982b-44ee-b288-4993e0eb72fc","_cell_guid":"1223c8bc-225c-4e37-b9d4-fd1b5757d7d7","trusted":true},"cell_type":"markdown","source":"Now, we'll open the image and predict:","execution_count":null},{"metadata":{"_uuid":"5b97fcbd-e7b8-4bd6-8f15-473c21c9adf3","_cell_guid":"75adfa76-0871-43b3-a440-8d778c169dff","trusted":true},"cell_type":"code","source":"from PIL import Image\nfrom pathlib import Path\nimage = Image.open(Path('./plastic.jpg'))\n\nexample_image = transformations(image)\nplt.imshow(example_image.permute(1, 2, 0))\npredict_image(example_image, loaded_model)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ab72121-ae1d-4f61-a16a-5ec8e4431cb1","_cell_guid":"0659b8ea-75ad-4169-a746-7eae47b8b59f","trusted":true},"cell_type":"code","source":"!pip install jovian --upgrade","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e8d908a-bb62-4f3d-ac58-3b04ed310a65","_cell_guid":"45d1bad7-2531-42a9-aa0b-000670790e67","trusted":true},"cell_type":"code","source":"import jovian","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"202cfbcd-3433-42bf-8c12-508ef462ff12","_cell_guid":"d546a7df-9b86-4c9c-ad97-776911bc0687","trusted":true},"cell_type":"code","source":"jovian.commit(project='garbage-classification')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa5738d5-31b8-4ff9-b78f-03e8b986e0ba","_cell_guid":"ec68f016-defc-4438-accd-e02c35dbeb6f","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}